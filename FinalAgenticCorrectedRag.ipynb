{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c519cf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\hf-gpu\\lib\\site-packages\\langchain_pinecone\\__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing modules which is used in simple RAG Project.\n",
    "#below classes we used so user can interact with LLM Models.\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint #HFE class we used to hit user query and take response from it.\n",
    "\n",
    "#below classes we used for embedding Models\n",
    "from langchain_openai import OpenAIEmbeddings  #close source model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings #open source model\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "#want to load document into workingspace using document loaders\n",
    "from langchain_community.document_loaders import PDFPlumberLoader,TextLoader\n",
    "\n",
    "#now splitting the document into chunks need splitter class\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter #this split the text document through herirachy way.\n",
    "\n",
    "#need to store the chunks embedded document to vector store we r using Pinecone.\n",
    "from pinecone import Pinecone,ServerlessSpec\n",
    "\n",
    "#integrating pinecode with langchain.\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langsmith import traceable\n",
    "\n",
    "#load the env files\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Annotated,Optional,List,Literal,TypedDict\n",
    "from dataclasses import dataclass\n",
    "from loggers import logger\n",
    "from Exception import CustomException\n",
    "import os,sys\n",
    "\n",
    "from langgraph.graph import StateGraph,START,END #using this class we can create Graph start or end of workflow\n",
    "\n",
    "#if i want to add tool support to my workflow.\n",
    "from langchain.tools import tool,Tool,StructuredTool\n",
    "\n",
    "#if i want to add toolnode in my workflow \n",
    "#(toolnode means that node have list of tool here they will decide based on user query which tool need to execute)\n",
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "\n",
    "#if i need to add memory or persistence to my workflow so that it can save the state value at every checkpoint\n",
    "from langgraph.checkpoint.memory import InMemorySaver #it will save in state value to Ram memory.\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "#tools_condition will  decide if tool message is present on AI response then they redirect to toolnode or end it workflow\n",
    "from pydantic import BaseModel,Field,computed_field\n",
    "\n",
    "#this class we used to change retriever object to become tool\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "#fetching the RAG prompt from Hub.\n",
    "from langchain import hub\n",
    "\n",
    "import warnings as w\n",
    "w.filterwarnings('ignore')\n",
    "\n",
    "from langchain_core.messages import AIMessage,HumanMessage,AnyMessage,ToolMessage\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1decaedd",
   "metadata": {},
   "source": [
    "## step:1) model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a73df2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groq model\n",
    "model1 = ChatGroq(\n",
    "    model=\"groq/compound-mini\",\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "#openai model\n",
    "model2 = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",temperature=0.1\n",
    ")\n",
    "\n",
    "#hugging face model.\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"meta-llama/Llama-3.1-8B-Instruct\",  \n",
    "    task=\"text-generation\",  \n",
    "    \n",
    ")\n",
    "model3 = ChatHuggingFace(llm=llm)\n",
    "\n",
    "\n",
    "emb_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b6036",
   "metadata": {},
   "source": [
    "## Setting Up Pincone Database to store Embedding Vector into Index Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a959ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medical-book-index']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pinecone Basic Configuration.\n",
    "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"medical-book-index\"\n",
    "\n",
    "# üîπ Ensure index exists\n",
    "existing_indexes = [idx[\"name\"] for idx in pc.list_indexes()]\n",
    "existing_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e34aea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 16:06:11,425]-INFO-13-‚ÑπÔ∏è Index medical-book-index already exists. Skipping creation.\n"
     ]
    }
   ],
   "source": [
    "if index_name not in existing_indexes:\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=1536,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
    "        tags={\"environment\": \"RAGdevelopment\"}\n",
    "    )\n",
    "    logger.info(f\"üÜï Created Pinecone index: {index_name}\")\n",
    "    import time\n",
    "    time.sleep(10)  # wait for index to be ready\n",
    "else:\n",
    "    logger.info(f\"‚ÑπÔ∏è Index {index_name} already exists. Skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26a99807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_pinecone.vectorstores.PineconeVectorStore at 0x1d1a2816e60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "vector_store = PineconeVectorStore(index=index, embedding=emb_model)\n",
    "vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d3129",
   "metadata": {},
   "source": [
    "### step:2) changing vector store to become as retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2db9fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 2, \"lambda_mult\": 0.25} #lambda_mult will give more diversified output using MMR algorithm\n",
    ")\n",
    "base_retriever #retrievers is runnable so it means we can invoke easily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff47c1",
   "metadata": {},
   "source": [
    "### step:3) Now changing vector store retriever to become tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9edb411d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tool(name='medical_doc_retriever', description='Use this tool to retrieve the most relevant information from the ingested PDFs. Best suited for answering questions that require factual context or reference to the uploaded documents.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001D1C6715AB0>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001D1C6715D80>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=base_retriever,\n",
    "    name=\"medical_doc_retriever\",\n",
    "    description=(\n",
    "        \"Use this tool to retrieve the most relevant information \"\n",
    "        \"from the ingested PDFs. Best suited for answering \"\n",
    "        \"questions that require factual context or reference to \"\n",
    "        \"the uploaded documents.\"\n",
    "    )\n",
    ")\n",
    "retriever_tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92cf677",
   "metadata": {},
   "source": [
    "#### Note :- adding more tools to support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ac0253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculator tool\n",
    "def calculator(first_num: float, second_num: float, operation: str) -> dict:\n",
    "    try:\n",
    "        if operation == \"add\": result = first_num + second_num\n",
    "        elif operation == \"sub\": result = first_num - second_num\n",
    "        elif operation == \"mul\": result = first_num * second_num\n",
    "        elif operation == \"div\": result = first_num / second_num if second_num != 0 else \"Division by zero\"\n",
    "        else: return {\"error\": f\"Unsupported operation {operation}\"}\n",
    "        return {\"first_num\": first_num, \"second_num\": second_num, \"operation\": operation, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "calc_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=calculator,\n",
    "    description=\"Perform basic arithmetic: add, sub, mul, div\"\n",
    ")\n",
    "\n",
    "# Stock price tool\n",
    "import requests\n",
    "def get_stock_price(symbol: str) -> dict:\n",
    "    url = f'https://www.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={symbol}&apikey=1PPNPDOMK62HNKRO'\n",
    "    return requests.get(url).json()\n",
    "\n",
    "stock_tool = Tool(\n",
    "    name=\"StockPrice\",\n",
    "    func=get_stock_price,\n",
    "    description=\"Fetch latest stock price for a given symbol\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b00ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tool(name='medical_doc_retriever', description='Use this tool to retrieve the most relevant information from the ingested PDFs. Best suited for answering questions that require factual context or reference to the uploaded documents.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001D1C6715AB0>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001D1C6715D80>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')),\n",
       " Tool(name='StockPrice', description='Fetch latest stock price for a given symbol', func=<function get_stock_price at 0x000001D1C7F6D2D0>),\n",
       " Tool(name='Calculator', description='Perform basic arithmetic: add, sub, mul, div', func=<function calculator at 0x000001D1C7F6D240>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now combining all tolls together\n",
    "lst_tools = [retriever_tool,stock_tool,calc_tool]\n",
    "lst_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "107c7aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001D1C697A800>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001D1C6979F90>, root_client=<openai.OpenAI object at 0x000001D1C697A2C0>, root_async_client=<openai.AsyncOpenAI object at 0x000001D1C7D57340>, temperature=0.1, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'tools': [{'type': 'function', 'function': {'name': 'medical_doc_retriever', 'description': 'Use this tool to retrieve the most relevant information from the ingested PDFs. Best suited for answering questions that require factual context or reference to the uploaded documents.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'StockPrice', 'description': 'Fetch latest stock price for a given symbol', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'Calculator', 'description': 'Perform basic arithmetic: add, sub, mul, div', 'parameters': {'properties': {'__arg1': {'title': '__arg1', 'type': 'string'}}, 'required': ['__arg1'], 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now binding the tool with LLM Model.\n",
    "llm_with_tool  = model2.bind_tools(tools=lst_tools)\n",
    "llm_with_tool  #tools is also runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb02987",
   "metadata": {},
   "source": [
    "## step:4) Defining State or Memory Schema that hold value throughout Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7c646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph.message import MessagesState #this message state is prebuilt class that store mesaage in lst \n",
    "from langgraph.graph.message import add_messages,BaseMessage\n",
    "\n",
    "class StateSchema(TypedDict):\n",
    "    #key schema defining i will store all the messages init as well as Query.\n",
    "    messages : Annotated[list[BaseMessage],add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3977485d",
   "metadata": {},
   "source": [
    "### step:5) Defineing the graph object and adding nodes and edges to graph so finally it ready the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ebfc848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d1c7f8f0a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating an object of stategraph class\n",
    "graph = StateGraph(state_schema=StateSchema)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3130d",
   "metadata": {},
   "source": [
    "#### adding nodes and edges to my graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating user_query_or_respond function that perform action in node.\n",
    "@traceable(\n",
    "    name=\"user_query_or_respond_func\",\n",
    "    run_type=\"ParalleltoolsInvokeBasedUseryQuery\",\n",
    "    metadata={\n",
    "        \"module\": \"rag_agent\",\n",
    "        \"owner\": \"Raees\", \n",
    "        \"description\": \"Decides whether to use a tool or respond directly\",\n",
    "        \"version\": \"v1.0\"\n",
    "    },tags=[\"stateMsg\",\"LLMBindWithtool\",\"updateState\"]\n",
    ")\n",
    "def user_query_or_respond(state:StateSchema) ->StateSchema:\n",
    "    \"\"\"Call the toool binded model to generate a response based on the current state(user query). Given\n",
    "    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n",
    "    \"\"\"\n",
    "    print(\"user_query_or_respond\")\n",
    "    #fetching the user query from state class.\n",
    "    human_msg = state['messages'] #taking user input\n",
    "    \n",
    "    #now sending this user query that is LLM who is bind with tools.\n",
    "    #it will parallely call all the tools together based on user query give suggestions which tool is suitable.\n",
    "    ai_response = llm_with_tool.invoke(input=human_msg)\n",
    "    \n",
    "    #now updating the partial state message.\n",
    "    return {\n",
    "        'messages' : [ai_response]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e4affc",
   "metadata": {},
   "source": [
    "#### Tool nodes will have all tool support init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe37bb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tools(tags=None, recurse=True, explode_args=False, func_accepts={'config': ('N/A', <class 'inspect._empty'>), 'store': ('store', None)}, tools_by_name={'medical_doc_retriever': Tool(name='medical_doc_retriever', description='Use this tool to retrieve the most relevant information from the ingested PDFs. Best suited for answering questions that require factual context or reference to the uploaded documents.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001D1C6715AB0>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001D1C6715D80>, retriever=VectorStoreRetriever(tags=['PineconeVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_pinecone.vectorstores.PineconeVectorStore object at 0x000001D1A2816E60>, search_type='mmr', search_kwargs={'k': 2, 'lambda_mult': 0.25}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content')), 'StockPrice': Tool(name='StockPrice', description='Fetch latest stock price for a given symbol', func=<function get_stock_price at 0x000001D1C7F6D2D0>), 'Calculator': Tool(name='Calculator', description='Perform basic arithmetic: add, sub, mul, div', func=<function calculator at 0x000001D1C7F6D240>)}, tool_to_state_args={'medical_doc_retriever': {}, 'StockPrice': {}, 'Calculator': {}}, tool_to_store_arg={'medical_doc_retriever': None, 'StockPrice': None, 'Calculator': None}, handle_tool_errors=True, messages_key='messages')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if ai response coming from user_query_or_respond node have Toolmessage toh will redirect to toolnode.\n",
    "#so defining logical that perform action in toolnode.\n",
    "tools = ToolNode(tools=lst_tools)\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c686cbf1",
   "metadata": {},
   "source": [
    "```\n",
    "## Very Important Meaning\n",
    "üëâ Grading documents ka matlab hai:\n",
    "LLM (ya ek heuristic) ka use karke har retrieved document ko evaluate karna ki:\n",
    "1)Kya yeh user query ke liye retrieve document relevant hai?\n",
    "2)Kya isme user ko answer dene ke liye sahi context hai?\n",
    "3)Agar multiple docs aaye to kaunsa sabse useful document hai?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0367b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define GrageDocument schema using pydantic.\n",
    "class GradeDocumentSchema(BaseModel):\n",
    "    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5916027f",
   "metadata": {},
   "source": [
    "#### messages[0] ‚Üí The original user question.\n",
    "#### messages[1] ‚Üí based on user question the assistant's placeholder,suggest which tool useful to solve the user question\n",
    "#### messages[2] ‚Üí The tool response (the document retrieved)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0cd1ef",
   "metadata": {},
   "source": [
    "# Router Function based grade document suggestion it redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95227017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "@traceable(\n",
    "    name=\"grade_document_func\",\n",
    "    run_type=\"RouterFucnDecideDocRelatedQueryBasedOnRedirecting\",\n",
    "    metadata={\n",
    "        \"module\": \"rag_agent\",\n",
    "        \"owner\": \"Raees\", \n",
    "        \"description\": \"Decidesto  whether a retrieved document is relevant to a given user question.\",\n",
    "        \"version\": \"v1.0\"\n",
    "    },tags=[\"stateMsg\",\"StructureBindingLLM\",\"updateState\"]\n",
    ")\n",
    "\n",
    "def grade_document(state: StateSchema) -> Literal['generate_answer','rewrite_question']:\n",
    "    model2_structure = model2.with_structured_output(GradeDocumentSchema)\n",
    "    \n",
    "    question = state['messages'][0].content\n",
    "    context = state['messages'][-1].content\n",
    "    \n",
    "    print(\"grade_document executed\")\n",
    "    # print(question)\n",
    "    # print(context)\n",
    "    \n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are a relevance grader. \n",
    "        Your task is to decide whether a retrieved document is relevant to a given user question.\n",
    "\n",
    "        Retrieved document:\n",
    "        {context}\n",
    "\n",
    "        User question:\n",
    "        {question}\n",
    "\n",
    "        Instructions:\n",
    "        - Consider both exact keyword matches and semantic meaning.\n",
    "        - If the document provides information that answers or is directly related to the question, grade it as 'yes'.\n",
    "        - If the document does not provide relevant information, grade it as 'no'.\n",
    "        Return the answer strictly in this format:\n",
    "        \"\"\",\n",
    "        input_variables=[\"context\",\"question\"],\n",
    "    )\n",
    "    \n",
    "    # Render the prompt into a string\n",
    "    prompt_text = prompt.format(context=context, question=question)\n",
    "    \n",
    "    # Pass as HumanMessage in a list\n",
    "    response = model2_structure.invoke([HumanMessage(content=prompt_text,name='Human')])\n",
    "    \n",
    "    #print(response.binary_score)\n",
    "    \n",
    "    score = response.binary_score\n",
    "    if score == \"yes\":\n",
    "        print('relevant document')\n",
    "        return \"generate_answer\"\n",
    "    else:\n",
    "        print('ir-relevant document')\n",
    "        return \"rewrite_question\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550542a6",
   "metadata": {},
   "source": [
    "##### Generate response if document in relevant based on user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246882b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating function for generate node that perform action init.\n",
    "@traceable(\n",
    "    name=\"generate_answer_func\",\n",
    "    run_type=\"GeneratingAnswer\",\n",
    "    metadata={\n",
    "        \"module\": \"rag_agent\",\n",
    "        \"owner\": \"Raees\", \n",
    "        \"description\": \"Generating Response based on User Query if document is relevant\",\n",
    "        \"version\": \"v1.0\"\n",
    "    },tags=[\"stateMsg\",\"GenerateAnswerLLM\",\"updateState\"]\n",
    ")\n",
    "def generate_answer(state:StateSchema)->StateSchema:\n",
    "    print(\"geenerate answer executing\")\n",
    "    #fetching the user question and context from state class\n",
    "    question = state[\"messages\"][0].content\n",
    "    context = state[\"messages\"][-1].content\n",
    "    \n",
    "    \n",
    "    #pulling the rag prompt from hub\n",
    "    from langchain_core.prompts import PromptTemplate\n",
    "    from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "    rag_prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        \n",
    "        You are an assistant for question-answering tasks. \n",
    "        Use the following pieces of retrieved context to answer the question. \n",
    "        If you don't know the answer, just say that you don't know. \n",
    "        Use three sentences maximum and keep the answer concise.\n",
    "        Question: {question} \n",
    "        Context: {context} \n",
    "        Answer:\n",
    "        \n",
    "        \"\"\",input_variables=['question','context']\n",
    "    )\n",
    "    \n",
    "    # Render the prompt into a string\n",
    "    prompt_text = rag_prompt.format(context=context, question=question)\n",
    "\n",
    "    result = model2.invoke([HumanMessage(content=prompt_text,name='Human')])\n",
    "    \n",
    "    #now updating to state memory\n",
    "    return {\n",
    "        'messages': [result]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b333dc",
   "metadata": {},
   "source": [
    "#### if document ir-relevant based on user query than rewrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff829d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "@traceable(\n",
    "    name=\"rewrite_question_func\",\n",
    "    run_type=\"RewritingQuestion\",\n",
    "    metadata={\n",
    "        \"module\": \"rag_agent\",\n",
    "        \"owner\": \"Raees\", \n",
    "        \"description\": \"if document is irrelevant based on question rewriting\",\n",
    "        \"version\": \"v1.0\"\n",
    "    },tags=[\"stateMsg\",\"questionRewriting\",\"updateState\"]\n",
    ")\n",
    "\n",
    "# creating function for rewrite node\n",
    "def rewrite_question(state: StateSchema) -> StateSchema:\n",
    "    print(\"rewrite question executing\")\n",
    "    # fetching user question\n",
    "    question = state['messages'][0].content\n",
    "\n",
    "    # creating structured instruction prompt to rewrite\n",
    "    hum_msg = ChatPromptTemplate(\n",
    "        messages=[\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"You are a helpful assistant that rewrites user questions to improve clarity and capture the true semantic intent.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"\"\"Look at the input and try to reason about the underlying semantic intent or meaning.\n",
    "                    Here is the initial question:\n",
    "                    -------\n",
    "                    {question}\n",
    "                    -------\n",
    "                    Formulate an improved question:\"\"\"\n",
    "                                ),\n",
    "                            ]\n",
    "    )\n",
    "    # Render the prompt into a string\n",
    "    prompt_text = hum_msg.format(question=question)\n",
    "\n",
    "    # invoke chain with question\n",
    "    improved_question_response = model2.invoke([HumanMessage(content=prompt_text,name=\"Human\")])\n",
    "\n",
    "    #return the response to state.\n",
    "\n",
    "    return {\n",
    "        'messages':[improved_question_response]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3c7b5df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d1c7f8f0a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.add_node(node=\"understand_user_query_or_respond\",action=user_query_or_respond)\n",
    "graph.add_node(node = \"vectorretriever\", action=tools)\n",
    "graph.add_node(node=\"rewrite_question\",action=rewrite_question)\n",
    "graph.add_node(node = \"generate_answer\",action=generate_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f39478b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d1c7f8f0a0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding edges to graph.\n",
    "graph.add_edge(START,\"understand_user_query_or_respond\")\n",
    "graph.add_conditional_edges(\n",
    "    \"understand_user_query_or_respond\"\n",
    "    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n",
    "    ,tools_condition,\n",
    "    {\n",
    "        \"tools\":\"vectorretriever\",\n",
    "        END:END\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3cb9ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1d1c7f8f0a0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Edges taken after the `action` node is called.\n",
    "graph.add_conditional_edges(\n",
    "    \"vectorretriever\",\n",
    "    # router function will give 2 response in structure manner either doc is relevant or irrelevant.\n",
    "    grade_document,{'generate_answer':\"generate_answer\",'rewrite_question':'rewrite_question'}\n",
    ")\n",
    "\n",
    "graph.add_edge('generate_answer',END)\n",
    "graph.add_edge('rewrite_question','understand_user_query_or_respond')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63692060",
   "metadata": {},
   "source": [
    "### Testing the Agentic Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b0ae90d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_query_or_respond\n",
      "[2025-09-23 16:06:13,492]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-09-23 16:06:14,188]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "grade_document executed\n",
      "[2025-09-23 16:06:19,396]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "relevant document\n",
      "geenerate answer executing\n",
      "[2025-09-23 16:06:20,384]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='Tell me about Acne How we prevent from this Problem?', additional_kwargs={}, response_metadata={}, id='18c77599-3a1b-43fb-9af4-7318548602df'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_cG6u09C2GB5AkbWjxEIyz25h', 'function': {'arguments': '{\"query\":\"Acne prevention\"}', 'name': 'medical_doc_retriever'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 161, 'total_tokens': 181, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CIufVZOU8zO9EZQQBCLyknodFKdTL', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1bff4a20-3cb4-49f7-8540-f96088b14309-0', tool_calls=[{'name': 'medical_doc_retriever', 'args': {'query': 'Acne prevention'}, 'id': 'call_cG6u09C2GB5AkbWjxEIyz25h', 'type': 'tool_call'}], usage_metadata={'input_tokens': 161, 'output_tokens': 20, 'total_tokens': 181, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='isotretinoin. It can be controlled by proper treatment, ORGANIZATIONS\\nwith improvement taking two or more months. Acne American Academy of Dermatology. 930 N. Meacham Road,\\ntends to reappear when treatment stops,but spontaneous- P.O. Box 4014,Schaumburg,IL 60168-4014. (847) 330-\\nly improves over time. Inflammatory acne may leave 0230. <http://www.aad.org>.\\nscars that require further treatment.\\nMercedes McLaughlin\\nPrevention\\nAcne rosacea see Rosacea\\nThere are no sure ways to prevent acne,but the fol-\\nAcoustic neurinoma see Acoustic neuroma\\nlowing steps may be taken to minimize flare-ups:\\n‚Ä¢gentle washing of affected areas once or twice every day\\n‚Ä¢avoid abrasive cleansers\\n‚Ä¢use noncomedogenic makeup and moisturizers\\n‚Ä¢shampoo often and wear hair off face Acoustic neuroma\\n‚Ä¢eat a well-balanced diet, avoiding foods that trigger\\nDefinition\\nflare-ups\\nAn acoustic neuroma is a benign tumor involving\\n‚Ä¢unless told otherwise, give dry pimples a limited\\n\\nexample, Ortho-Tri-Cyclen) and female sex hormones\\nKEY TERMS (estrogens) reduce hormone activity in the ovaries. Other\\ndrugs,for example,spironolactone and corticosteroids,\\nAndrogens‚ÄîMale sex hormones that are linked reduce hormone activity in the adrenal glands. Improve-\\nwith the development of acne. ment may take up to four months.\\nAntiandrogens‚ÄîDrugs that inhibit the production Oral corticosteroids,or anti-inflammatory drugs,are\\nof androgens. the treatment of choice for an extremely severe,but rare\\ntype of destructive inflammatory acne called acne fulmi-\\nAntibiotics‚ÄîMedicines that kill bacteria.\\nnans,found mostly in adolescent males. Acne congloba-\\nComedo‚ÄîA hard plug composed of sebum and ta,a more common form of severe inflammation,is char-\\ndead skin cells. The mildest type of acne. acterized by numerous,deep,inflammatory nodules that\\nComedolytic‚ÄîDrugs that break up comedones heal with scarring. It is treated with oral isotretinoin and\\nand open clogged pores. corticosteroids.', name='medical_doc_retriever', id='ddf2ddff-38b0-4c7f-9113-80b2112d787f', tool_call_id='call_cG6u09C2GB5AkbWjxEIyz25h'),\n",
       "  AIMessage(content='Acne can be prevented by gentle washing of affected areas, avoiding abrasive cleansers, using noncomedogenic makeup and moisturizers, shampooing often, and eating a well-balanced diet. Some medications like oral contraceptives and anti-inflammatory drugs can also help prevent acne. Improvement may take time, and severe cases may require treatments like isotretinoin and corticosteroids.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 77, 'prompt_tokens': 579, 'total_tokens': 656, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CIufca8f3kURg9lT3ssbc5XlgNtz5', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c81ebd86-6a50-4676-b115-715eed2dd439-0', usage_metadata={'input_tokens': 579, 'output_tokens': 77, 'total_tokens': 656, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = graph.compile()\n",
    "messages=[HumanMessage(content=\"Tell me about Acne How we prevent from this Problem?\")]\n",
    "\n",
    "#passing this state to workflow.\n",
    "response = workflow.invoke({'messages':messages},config={\"run_name\": \"AgenticRAGWorkflow\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5baa844e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Tell me about Acne How we prevent from this Problem?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  medical_doc_retriever (call_cG6u09C2GB5AkbWjxEIyz25h)\n",
      " Call ID: call_cG6u09C2GB5AkbWjxEIyz25h\n",
      "  Args:\n",
      "    query: Acne prevention\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: medical_doc_retriever\n",
      "\n",
      "isotretinoin. It can be controlled by proper treatment, ORGANIZATIONS\n",
      "with improvement taking two or more months. Acne American Academy of Dermatology. 930 N. Meacham Road,\n",
      "tends to reappear when treatment stops,but spontaneous- P.O. Box 4014,Schaumburg,IL 60168-4014. (847) 330-\n",
      "ly improves over time. Inflammatory acne may leave 0230. <http://www.aad.org>.\n",
      "scars that require further treatment.\n",
      "Mercedes McLaughlin\n",
      "Prevention\n",
      "Acne rosacea see Rosacea\n",
      "There are no sure ways to prevent acne,but the fol-\n",
      "Acoustic neurinoma see Acoustic neuroma\n",
      "lowing steps may be taken to minimize flare-ups:\n",
      "‚Ä¢gentle washing of affected areas once or twice every day\n",
      "‚Ä¢avoid abrasive cleansers\n",
      "‚Ä¢use noncomedogenic makeup and moisturizers\n",
      "‚Ä¢shampoo often and wear hair off face Acoustic neuroma\n",
      "‚Ä¢eat a well-balanced diet, avoiding foods that trigger\n",
      "Definition\n",
      "flare-ups\n",
      "An acoustic neuroma is a benign tumor involving\n",
      "‚Ä¢unless told otherwise, give dry pimples a limited\n",
      "\n",
      "example, Ortho-Tri-Cyclen) and female sex hormones\n",
      "KEY TERMS (estrogens) reduce hormone activity in the ovaries. Other\n",
      "drugs,for example,spironolactone and corticosteroids,\n",
      "Androgens‚ÄîMale sex hormones that are linked reduce hormone activity in the adrenal glands. Improve-\n",
      "with the development of acne. ment may take up to four months.\n",
      "Antiandrogens‚ÄîDrugs that inhibit the production Oral corticosteroids,or anti-inflammatory drugs,are\n",
      "of androgens. the treatment of choice for an extremely severe,but rare\n",
      "type of destructive inflammatory acne called acne fulmi-\n",
      "Antibiotics‚ÄîMedicines that kill bacteria.\n",
      "nans,found mostly in adolescent males. Acne congloba-\n",
      "Comedo‚ÄîA hard plug composed of sebum and ta,a more common form of severe inflammation,is char-\n",
      "dead skin cells. The mildest type of acne. acterized by numerous,deep,inflammatory nodules that\n",
      "Comedolytic‚ÄîDrugs that break up comedones heal with scarring. It is treated with oral isotretinoin and\n",
      "and open clogged pores. corticosteroids.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Acne can be prevented by gentle washing of affected areas, avoiding abrasive cleansers, using noncomedogenic makeup and moisturizers, shampooing often, and eating a well-balanced diet. Some medications like oral contraceptives and anti-inflammatory drugs can also help prevent acne. Improvement may take time, and severe cases may require treatments like isotretinoin and corticosteroids.\n"
     ]
    }
   ],
   "source": [
    "for msg in response['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b2235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7463b3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_query_or_respond\n",
      "[2025-09-23 16:06:21,587]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "[2025-09-23 16:06:22,100]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "grade_document executed\n",
      "[2025-09-23 16:06:25,873]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "ir-relevant document\n",
      "rewrite question executing\n",
      "[2025-09-23 16:06:26,502]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "user_query_or_respond\n",
      "[2025-09-23 16:06:27,659]-INFO-1025-HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What is langgraph how it work?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I can help you find information on Langgraph and how it works. Let me retrieve relevant details about Langgraph for you.\n",
      "Tool Calls:\n",
      "  medical_doc_retriever (call_GUuJQ92xeZVCn6zx3pMpNSUp)\n",
      " Call ID: call_GUuJQ92xeZVCn6zx3pMpNSUp\n",
      "  Args:\n",
      "    query: Langgraph\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: medical_doc_retriever\n",
      "\n",
      "cessing,and recall.(Illustration by Electronic Illustrators Group).\n",
      "\n",
      "used to detect aneurysm, thrombosis, and signs of\n",
      "ischemia in the celiac and mesenteric arteries, and to\n",
      "Pulmonary angiography locate the source of gastrointestinal bleeding. It is also\n",
      "used in the diagnosis of a number of conditions,includ-\n",
      "Pulmonary, or lung, angiography is performed to\n",
      "ing portal hypertension,and cirrhosis. The procedure\n",
      "evaluate blood circulation to the lungs. It is also consid-\n",
      "can take up to three hours,depending on the number of\n",
      "ered the most accurate diagnostic test for detecting a pul-\n",
      "blood vessels studied.\n",
      "monary embolism. The procedure differs from cerebral\n",
      "and coronary angiograms in that the guide wire and\n",
      "Splenoportography\n",
      "catheter are inserted into a vein instead of an artery,and\n",
      "are guided up through the chambers of the heart and into A splenoportograph is a variation of an angiogram\n",
      "the pulmonary artery. Throughout the procedure, the that involves the injection of contrast medium directly\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "What is Langgraph and how does it work?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Langgraph is a medical imaging technique used to detect aneurysm, thrombosis, signs of ischemia in the celiac and mesenteric arteries, and to locate the source of gastrointestinal bleeding. It is also used in the diagnosis of conditions such as portal hypertension and cirrhosis. Langgraph involves the injection of contrast medium directly into the blood vessels to visualize and study the circulation in specific areas of the body. The procedure can take up to three hours, depending on the number of blood vessels studied. Langgraph is a valuable tool in diagnosing and evaluating various vascular conditions.\n"
     ]
    }
   ],
   "source": [
    "messages=[HumanMessage(content=\"What is langgraph how it work?\")]\n",
    "\n",
    "#passing this state to workflow.\n",
    "response = workflow.invoke({'messages':messages})\n",
    "for msg in response['messages']:\n",
    "    msg.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "128885c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-09-23 16:06:28,806]-ERROR-41-Python Error getting in this file C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9456\\2086259708.py at Line No 4 and Message Of Error is Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n",
      "\n",
      "To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    },
    {
     "ename": "CustomException",
     "evalue": "Python Error getting in this file C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9456\\2086259708.py at Line No 4 and Message Of Error is Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     display(Image(\u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxray\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\hf-gpu\\lib\\site-packages\\langchain_core\\runnables\\graph.py:702\u001b[0m, in \u001b[0;36mGraph.draw_mermaid_png\u001b[1;34m(self, curve_style, node_colors, wrap_label_n_words, output_file_path, draw_method, background_color, padding, max_retries, retry_delay, frontmatter_config)\u001b[0m\n\u001b[0;32m    696\u001b[0m mermaid_syntax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraw_mermaid(\n\u001b[0;32m    697\u001b[0m     curve_style\u001b[38;5;241m=\u001b[39mcurve_style,\n\u001b[0;32m    698\u001b[0m     node_colors\u001b[38;5;241m=\u001b[39mnode_colors,\n\u001b[0;32m    699\u001b[0m     wrap_label_n_words\u001b[38;5;241m=\u001b[39mwrap_label_n_words,\n\u001b[0;32m    700\u001b[0m     frontmatter_config\u001b[38;5;241m=\u001b[39mfrontmatter_config,\n\u001b[0;32m    701\u001b[0m )\n\u001b[1;32m--> 702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw_mermaid_png\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdraw_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdraw_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    710\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\hf-gpu\\lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:310\u001b[0m, in \u001b[0;36mdraw_mermaid_png\u001b[1;34m(mermaid_syntax, output_file_path, draw_method, background_color, padding, max_retries, retry_delay)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m draw_method \u001b[38;5;241m==\u001b[39m MermaidDrawMethod\u001b[38;5;241m.\u001b[39mAPI:\n\u001b[1;32m--> 310\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m \u001b[43m_render_mermaid_using_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmermaid_syntax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbackground_color\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackground_color\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\hf-gpu\\lib\\site-packages\\langchain_core\\runnables\\graph_mermaid.py:463\u001b[0m, in \u001b[0;36m_render_mermaid_using_api\u001b[1;34m(mermaid_syntax, output_file_path, background_color, file_type, max_retries, retry_delay)\u001b[0m\n\u001b[0;32m    459\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    460\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reach https://mermaid.ink/ API while trying to render \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour graph. Status code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    462\u001b[0m     ) \u001b[38;5;241m+\u001b[39m error_msg_suffix\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (requests\u001b[38;5;241m.\u001b[39mRequestException, requests\u001b[38;5;241m.\u001b[39mTimeout) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mCustomException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     display(Image(workflow\u001b[38;5;241m.\u001b[39mget_graph(xray\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CustomException(e,sys)\n",
      "\u001b[1;31mCustomException\u001b[0m: Python Error getting in this file C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_9456\\2086259708.py at Line No 4 and Message Of Error is Failed to reach https://mermaid.ink/ API while trying to render your graph. Status code: 502.\n\nTo resolve this issue:\n1. Check your internet connection and try again\n2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "workflow = graph.compile()\n",
    "try:\n",
    "    display(Image(workflow.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
